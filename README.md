
# **AI Chatbot based on QA-LSTMğŸ¤–**

  

### **2016 ICLR LSTM-based Deep Learning Models for Non-factoid Answer Selection**

QA-LSTMê³¼ QA-LSTM with Attention ëª¨ë¸ êµ¬í˜„ (`avg_pooling`/`max_pooling`)

 
<br>

-----
  

### ğŸ‘‰ **Word Embedding**

&nbsp;&nbsp;&nbsp;&nbsp;**Pretrained Embedding**: KoBERT `monologg/kobert`ì„ ì‚¬ìš©í•œ BERT ì„ë² ë”©

```
python main.py <ìƒëµ> --embed bert
```


&nbsp;&nbsp;&nbsp;&nbsp;**Embedding Layer**: nn.Embeddingì„ ì´ìš©í•œ ì„ë² ë”©

```
python main.py <ìƒëµ> --embed nn
```
<br>
<br>

-----
### **ëª¨ë¸ í”„ë ˆì„ì›Œí¬**

<br>
<div  align=left>
<img  src="./img/QA-LSTM.png"  width=700/><br>
QA-LSTM
</div>
<br>


<br>
<div  align=left>
<img  src="./img/QA-LSTM_attn.png"  width=700/><br>
QA-LSTM with attention
</div>
<br>

  
  

ğŸ‘‰ **Attention mechanism**  
&nbsp;&nbsp;Bahdanau Attention mechanism ì‚¬ìš©

<br>

-----

### **ëª¨ë¸ í›ˆë ¨**
**Multi GPU**
```
python main.py --cuda --gpuid [list of gpuid] --data_dir [data dir path] --method [pooling type] --embed [embedding method] --max_epochs 10 --train --model_name [model_name] --accelerator ddp
```

**Single GPU**
```
python main.py --cuda --gpuid 0 --data_dir [data directory path] --method [pooling type] --embed [embedding method] --max_epochs 10 --train --model_name [model_name]
```

- data_dir: `train.csv`, `val.csv`ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
- method: `avg_pooling` or `max_pooling`
- gpuid: GPU ID ë¦¬ìŠ¤íŠ¸
	- ex) `--gpuid 0 1 2`
- embed: 
	- `bert` : ì‚¬ì „í•™ìŠµëœ KoBERT ì„ë² ë”© (`monologg/kobert` ì‚¬ìš©)
	- `nn` : torch.nn.Embedding Layer  

### **ëª¨ë¸ ê²€ì¦**
ğŸ“£ ê²€ì¦ ì‹œ `LightningQALSTM`ì˜ `embd_size`, `hiddend_size` í™•ì¸ í•„ìš”  (í›ˆë ¨ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •)  

```
python main.py --cuda --model_pt [model path] --gpuid [gpu id] --data_dir [data directory path]
```

- data_dir: `reaction_emb.pickle`ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ìƒì„±)
- method: `avg_pooling` or `max_pooling` (í•™ìŠµëœ ëª¨ë¸ê³¼ ë™ì¼í•œ pooling method ì„ íƒ)
- model_pt: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ 
	- ex) `--model_pt model_ckpt/qa_lstm-epoch\=04-train_loss\=0.05.ckpt`
- gpuid: í•˜ë‚˜ì˜ GPU ID 
- embed: 
	- `bert` : ì‚¬ì „í•™ìŠµëœ KoBERT ì„ë² ë”© (`monologg/kobert` ì‚¬ìš©)
	- `nn` : torch.nn.Embedding Layer  

#### **ëª¨ë¸ ê²€ì¦ ì˜ˆì‹œ**


<br>

> **Model Info**   
>&nbsp;&nbsp;&nbsp;&nbsp;Model : QA-LSTM  
&nbsp;&nbsp;&nbsp;&nbsp;pooling : max_pooling  
&nbsp;&nbsp;&nbsp;&nbsp;embedding method: nn.Embedding Layer   
&nbsp;&nbsp;&nbsp;&nbsp;embedding size: 256    
&nbsp;&nbsp;&nbsp;&nbsp;hiddend size: 128     

<div  align=left>
<img  src="./img/example.png"  width=700/><br>
</div>
<br>

<br>

> **Model Info**   
>&nbsp;&nbsp;&nbsp;&nbsp;Model : QA-LSTM with attention   
&nbsp;&nbsp;&nbsp;&nbsp;pooling : max_pooling    
&nbsp;&nbsp;&nbsp;&nbsp;embedding method: nn.Embedding Layer   
&nbsp;&nbsp;&nbsp;&nbsp;embedding size: 256    
&nbsp;&nbsp;&nbsp;&nbsp;hiddend size: 128     

<div  align=left>
<img  src="./img/example_attn.png"  width=700/><br>
</div>
<br>


