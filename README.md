
# **AI Chatbot based on QA-LSTMπ¤–**

  

### **2016 ICLR LSTM-based Deep Learning Models for Non-factoid Answer Selection**

QA-LSTMκ³Ό QA-LSTM with Attention λ¨λΈ κµ¬ν„ (`avg_pooling`/`max_pooling`)

 
<br>

-----
  

### π‘‰ **Word Embedding**

&nbsp;&nbsp;&nbsp;&nbsp;**Pretrained Embedding**: KoBERT `monologg/kobert`μ„ μ‚¬μ©ν• BERT μ„λ² λ”©

```
python main.py <μƒλµ> --embed bert
```


&nbsp;&nbsp;&nbsp;&nbsp;**Embedding Layer**: nn.Embeddingμ„ μ΄μ©ν• μ„λ² λ”©

```
python main.py <μƒλµ> --embed nn
```
<br>
<br>

-----
### **λ¨λΈ ν”„λ μ„μ›ν¬**

<br>
<div  align=left>
<img  src="./img/QA-LSTM.png"  width=700/><br>
QA-LSTM
</div>
<br>


<br>
<div  align=left>
<img  src="./img/QA-LSTM_attn.png"  width=700/><br>
QA-LSTM with attention
</div>
<br>

  
  

π‘‰ **Attention mechanism**  
&nbsp;&nbsp;Bahdanau Attention mechanism μ‚¬μ©

<br>

-----

### **λ¨λΈ ν›λ ¨**
**Multi GPU**
```
python main.py --cuda --gpuid [list of gpuid] --data_dir [data dir path] --method [pooling type] --embed [embedding method] --max_epochs 10 --train --model_name [model_name] --accelerator ddp
```

**Single GPU**
```
python main.py --cuda --gpuid 0 --data_dir [data directory path] --method [pooling type] --embed [embedding method] --max_epochs 10 --train --model_name [model_name]
```

- data_dir: `train.csv`, `val.csv`κ°€ μλ” λ””λ ‰ν† λ¦¬ κ²½λ΅
- method: `avg_pooling` or `max_pooling`
- gpuid: GPU ID λ¦¬μ¤νΈ
	- ex) `--gpuid 0 1 2`
- embed: 
	- `bert` : μ‚¬μ „ν•™μµλ KoBERT μ„λ² λ”© (`monologg/kobert` μ‚¬μ©)
	- `nn` : torch.nn.Embedding Layer  

### **λ¨λΈ κ²€μ¦**

```
python main.py --cuda --model_pt [model path] --gpuid [gpu id] --data_dir [data directory path]
```

- data_dir: `reaction_emb.pickle`μ΄ μλ” λ””λ ‰ν† λ¦¬ κ²½λ΅ (μ—†λ” κ²½μ° μƒλ΅ μƒμ„±)
- method: `avg_pooling` or `max_pooling` (ν•™μµλ λ¨λΈκ³Ό λ™μΌν• pooling method μ„ νƒ)
- model_pt: λ¨λΈ μ²΄ν¬ν¬μΈνΈ κ²½λ΅ 
	- ex) `--model_pt model_ckpt/qa_lstm-epoch\=04-train_loss\=0.05.ckpt`
- gpuid: ν•λ‚μ GPU ID 
- embed: 
	- `bert` : μ‚¬μ „ν•™μµλ KoBERT μ„λ² λ”© (`monologg/kobert` μ‚¬μ©)
	- `nn` : torch.nn.Embedding Layer  

#### **λ¨λΈ κ²€μ¦ μμ‹**


<br>
<div  align=left>
<img  src="./img/example.png"  width=700/><br>
</div>
<br>

> **Model Info**   
>&nbsp;&nbsp;&nbsp;&nbsp;Model : QA-LSTM  
&nbsp;&nbsp;&nbsp;&nbsp;pooling : max_pooling  
&nbsp;&nbsp;&nbsp;&nbsp;embedding method: nn.Embedding Layer  
